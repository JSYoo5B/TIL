# 어휘 분석

앞에서도 잠깐 강의 외적으로 분석해놨지만, 지금까지 배운 정규언어는 어휘를 분석하는 데 사용된다.

어휘를 분석한다는 것은, 긴 입력의 코드를 잘게 토큰 단위로 나누어 전달하는 것이다.  
만약 코드로 `a+2`가 주어졌을 때, 중간에 띄어쓰기가 있건 없건,

* `a`는 identifier임을 파악해야 하고
* `+`는 덧셈 연산임을 파악해야 하고
* `2`는 숫자 리터럴 상수임을 파악해야 한다.

## 토큰 분리

토큰은 임의의 단어일 수도 있고, 특정 기호의 조합일 수도 있고, 여러 정규언어를 연결한 것일 수도 있다.

* `if`, `while`, `for` 같은 단어를 언어의 키워드라 한다.  
  언어에서 특수하게 취급되는 단어이며, 코드의 흐름이나 규약을 정한다.
* `+`, `+=`, `==`, `<` 같은 기호들은 연산자로 취급한다.  
  대부분 언어에서 보편적으로 사용하는 연산의 의미로, 대수 연산, 비교 논리 연산 등을 표현한다.
* `{`, `}`, `(`, `)` 같은 기호들은 구분자로 취급한다.  
  직접적인 기능을 하진 않는 편이지만, 각 기호 간의 상관관계 등을 구분할 때 사용된다.

위 키워드와 기호들은 이미 고정되어 있으며, 지정된 토큰 나열 번호만으로도 서로를 구분할 수 있다.  
다만 아래와 같은 경우는 일반화된 입력으로, 프로그래머의 의도에 따라 어떤 값이 나올 지 모른다.

* `a`, `b` 와 같은 변수 명, 혹은 `add` 같은 함수명 등을 identifier라 한다.
* `0`, `0xFFABCD08`, `123.45+e6`, `"hello world"` 같은 코드상 작성된 상수 등을 literal이라 한다.

이 경우, identifier와 literal은 token의 한 종류지만, token의 내용이 필요하다.  
(추후 symbol table 연결, 상수 활용 연산 시 타입 검사 등 어휘 분석 단계보단 의미 분석 단계에서 활용됨)

위 내용에 따라 어휘 분석 과정을 통해 얻은 token은 아래 속성을 알아야 한다.

* type (enum)
* value (identifier string / literal string)
* (optional) position (line number, column number, length)

기본 토큰은 value가 필요 없으므로 값을 들고 있지 않아도 되지만, 일반화된 입력은 value값을 알고 있어야 한다.  
또한 어휘 분석의 핵심은 아니지만, 문법 검사 등에 대해 오류 보고를 하려면 어느 위치에 있는 코드가 문제인지 알려주는 것이 좋으므로, 토큰의 시작 지점, 길이 등을 보관하는 것이 좋다.

## 토큰 인식 과정

고정된 기호와 키워드들은 인식하기 너무 간단하다. 하지만 변수, 상수 등을 구분하는 것은 복잡하다.
이 때, 변수, 상수 등을 잘 구분해내기 위한 방법으로 정규언어, 유한 오토마타를 사용한다.  
보통은 수학적 정규 표현식이 아닌, 실무적으로 사용하는 [정규 표현식](https://devlog.jsyoo5b.net/ko/posts/regex/basic/)을 활용한다.

아래 각 정규표현식은 간략하게 의도만 표현한 것이며, 상수의 경우 부호나 세세한 부분은 일부 생략했다.

### identifier

보통 알파벳 혹은 `_`로 첫 글자를 시작하며, 그 다음 글자부터는 알파벳, `_`, 숫자가 나올 수 있다.  
(첫 글자에서만 숫자가 금지됨, 이런 이유는 뒤에 숫자 literal 표기 과정의 모호함을 제거하기 위함임)

`[a-zA-Z_][a-zA-Z0-9_]*`

### integer literal

숫자 표기법에는 크게 10진법, 8진법, 16진법, (가끔 2진법) 정수 표기와, 실수 표기법이 있다. 실수 표기는 다음에서 다룬다.  
보통 10진법과 8진법, 16진법 등을 다르게 구분하기 위해, 10진법은 `0`을 제외한 2자리수 이상에서 0을 사용하지 못하게 한다.

`(0|[1-9][0-9]*)`

8진법은 그냥 `0`으로 시작하는 경우도 있고, `0o`으로 시작하는 경우도 있다.

`0o?[0-7]+`

16진법은 보통 `0x`로 시작하는 편이며, 10~15를 각각 `A`~`F`로 표현하며, 알파벳의 경우 대소문자 구분을 하지 않는다.

`0x[0-9a-fA-F]+`

### floating point literal

실수 표기법의 경우, 소수점 자리수를 직접 표기하는 방법이 있고, 과학적 표기법을 통해 유효 소수점+자리수를 표현하는 경우가 있다.

`[0-9]*\.[0-9]*`

`[0-9]*\.[0-9]+e[+-]?[0-9]+`

### string literal

보통 문자열은 `""`으로 감싸서 표현하는 편이다. 가끔 `"` 기호 자체를 표현하기 위해 `\"`과 같이 백스페이스로 시작하여 escape를 하는 경우가 존재한다.

`"(\\"|[a-zA-Z0-9_ \t]*)"`

### comment

주석의 경우 C에서부터 시작된 `/* ~ */` 스타일의 범위 주석이 있고, `//`나 `#`으로 시작하여 해당 라인의 끝까지 주석하는 인라인 주석이 있다.

인라인 주석의 경우, 간단하게 해당 라인의 끝까지 주석으로 인식하면 된다.

`//.*$`

하지만 범위 주석의 경우, 주석이 끝나는 지점을 잘 찾아야 한다. 정규표현식의 `.*`을 잘못 썼다간, 코드 끝까지 인식할 수도 있기 때문에 lazy evaluation을 적용시켜야 한다.

`/\*.*?\*/`

## 어휘분석기 개발 방법

어휘 분석기는 위에서 설명한 정규표현식들에 대한 오토마타들을 직접 구현하는 방법도 있고, 자동회된 도구를 사용하는 방법도 있다. 자동화된 도구는 결국 주어진 규칙에 따라 오토마타를 구현하는 방식이긴 하다.

참고로 코드를 직접 구현할 때 주의해야 하는 부분은, 각 인식기나 토큰 종류 간에 우선순위 처리 등을 고려해야 한다는 것이다.

예를 들어, identifier 인식기를 우선적으로 적용하게 한다면, `if`, `while` 모두 identifier 인식기의 기준을 만족하게 되기 때문에 keyword로서 판단이 힘들어 지는 문제가 있다. 보통 이 경우, 최대 일치 원칙(Maximal Munch Principle)을 도입하여, 일단 identifier로서 인식했다가, 이후 keyword인지 추가 확인하는 과정을 거친다.

또한 `+`를 처음 인식했을 때, 그대로 끝낼 것이 아니라, 뒤까지 포함하여 `++`, `+=` 등의 수식은 아닌지 추가 검사가 필요하다. 보통 이 경우는 다음 글자를 미리 엿봄으로서 처리한다.

일단 해당 강의에서는 `lex - yacc`을 통해 어휘 분석기, 구문 분석기를 구현할 예정이며, 실제 패키지 설치는 `flex - bison`으로 검색해서 설치해야 한다.

### Lex 코드 작성

lex 코드인 `*.l` 파일을 아래와 같은 구성으로 작성하고, lex cli를 돌리면 `lex.yy.c` 파일이 생성된다.

```lex
definitions
%%
rules
%%
user subroutines
```

위 순서대로 코드를 작성하게 되어있다.

* definitions: C로 작성될 때 필요한 header include를 포함하여 C 코드로 작성할 기초적인 상수 등을 선언해둔다.  
  또한, 필요하다면 일부 정규표현식 들을 정리하여 다른 내부 심볼 등으로 표현한다.
* rules: 실제 토큰을 인식하기 위한 규칙을 서술한다.  
  위의 definitions에서 활용한 값을 반환하게 하거나, 혹은 definitions에서 정의한 정규표현식 심볼 등을 활용할 수 있다.
* user subroutines: 사용자가 작성하고 싶은 추가 함수 부분이 있다면 추가 작성한다.

이 부분은 일단 [다른 블로그](https://velog.io/@soopsaram/yacc-%EC%99%80-lex-%EB%A1%9C-%ED%8C%8C%EC%84%9C-%EB%A7%8C%EB%93%A4%EA%B8%B0)를 참고해봤다.